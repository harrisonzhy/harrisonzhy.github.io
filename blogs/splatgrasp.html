<!doctype html>
<html lang="en">

<head>

  <!-- Required meta tags -->
  <meta charset="utf-8" />
  <!-- <meta http-equiv="X-UA-Compatible" content="IE=edge"> -->
  <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">

  <meta name="description" content="Blog">
  <meta name="author" content="Harrison Zhang">

  <title>Harrison Zhang</title>

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">

  <!-- Bootstrap Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

  <!-- Academicons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <!-- My CSS -->
  <link href="../css/blog-style.css" rel="stylesheet">

  <!-- Bootstrap JS -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
      
  <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</head>

<body>

  <div class="container-fluid">

    <!-- Title -->
    <div class="sec-wrapper blog-wrapper" id="title">
      <div class="row align-items-center">
        <div class="col-sm col-wrapper">
          <div class="description">
            <div class="name">SplatGrasp: Language-Guided 3D Grasp Affordances from 2D Gaussian Splatting Reconstruction</div>
            <div class="blog-author">
              Juan Atehortúa*,
              Chikaha Tsuji*,
              <me>Harrison Zhang</me>*
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- Title -->


    <!-- Context -->
    <div class="sec-wrapper blog-sec-wrapper" id="enhancement">
      <div class="row section-head">
        <div class="col col-wrapper">
          <div class="section-title">Context</div>
        </div>
      </div>
      <div class="row section-item">
        <div class="col col-wrapper">
<p>
Work done as final project for <a href="https://manipulation.csail.mit.edu/Fall2024/">6.4212 (Graduate Robotic Manipulation)</a> at MIT.
</p>
        </div>
      </div>
    </div>
    <!-- Context ->


    <!-- Abstract -->
    <div class="sec-wrapper blog-sec-wrapper" id="abstract">
      <div class="row section-head">
        <div class="col col-wrapper">
          <div class="section-title">tl;dr</div>
        </div>
      </div>
      <div class="row section-item">
        <div class="col col-wrapper">
We develop <i>SplatGrasp</i>, a stack for language-guided 3D grasp affordance generation and robotic manipulation directly from casual smartphone captures. 
Leveraging recent advances in <a href="https://surfsplatting.github.io">2D Gaussian Splatting (2DGS)</a> for radiance field reconstruction, we generate high-fidelity object surfaces and stable depth maps. 
Next, we extract object-level and query-specific (e.g., “vase handle”) masks, project them into 3D via these depth maps, and fuse the language-information 
into <a href="https://www.open3d.org/docs/0.14.1/tutorial/t_reconstruction_system/integration.html">truncated signed distance field (TSDF)</a> voxel volumes to produce full-object and query-specific mesh 
assets that inform grasp generation. We show improved grasp specificity and success rates compared to geometry-only baselines, highlighting the 
promise of fusing geometric precision with open-vocabulary language understanding for robotic manipulation. 
        </div>
      </div>
    </div>
    <!-- Abstract -->

    <!-- Introduction -->
    <div class="sec-wrapper blog-sec-wrapper" id="enhancement">
      <div class="row section-head">
        <div class="col col-wrapper">
          <div class="section-title">Introduction</div>
        </div>
      </div>
      <div class="row section-item">
        <div class="col col-wrapper">
<p>
Modern robotic manipulation tasks demand not only reliable geometric reasoning but also semantic understanding. 
While geometry-focused approaches excel at finding stable grasps, they fail to incorporate user intent or contextual cues. 
Consider the instruction “grasp the vase by the handle”: a purely geometric planner may choose a stable but semantically irrelevant grasp. 
By pairing high-quality 2DGS geometric reconstruction with <a href="https://arxiv.org/abs/2201.03546">open-vocabulary segmentation aproaches</a>, 
we produce language-informed affordance maps to reconstruct full-object and query-specific mesh assets via 
<a href="https://github.com/andyzeng/tsdf-fusion">TSDF fusion</a>. This yields a semantically rich 3D representation, enabling language-driven grasp planning. 
Compared to geometry-only baselines, our approach enhances grasp relevance and reliability, demonstrated through multi-stage pick-and-place experiments.
</p>
        </div>
      </div>
    </div>
    <!-- Introduction -->


    <!-- Data Preparation -->
    <div class="sec-wrapper blog-sec-wrapper" id="integration">
        <div class="row section-head">
          <div class="col col-wrapper">
            <div class="section-title">Data Preparation</div>
          </div>
        </div>
        <div class="row section-item">
          <div class="col col-wrapper">
<p>
We begin with a set of $N$ images capturing the target object from multiple viewpoints. Using <a href="https://colmap.github.io">COLMAP</a>, 
we estimate camera parameters $(\mathbf{K}_n, \mathbf{R}_n, \mathbf{t}_n)$ for each view $n=1,\dots,N$, which are the 
intrinsic calibration matrix and rotation and translation from world to camera coordinates respectively. 
This provides a consistent reference frame.
</p>
            </div>
          </div>
        </div>
    <!-- Data Preparation -->


    <!-- 2DGS -->
    <div class="sec-wrapper blog-sec-wrapper" id="integration">
      <div class="row section-head">
        <div class="col col-wrapper">
          <div class="section-title">2D Gaussian Splatting (2DGS) Reconstruction</div>
        </div>
      </div>
      <div class="row section-item">
        <div class="col col-wrapper">
<p>
The 2DGS paradigm represents the scene with a set of $M$ planar Gaussian primitives defined 
by two tangent vectors $(\mathbf{t}_u, \mathbf{t}_v)$, scaling factors, and appearance parameters 
(e.g. opacity). Therefore, the constructed local coordinate system for each primitive is 
$\mathbf{R} = \left[ \mathbf{t}_u, \mathbf{t}_v,  (\mathbf{t}_u \times \mathbf{t}_v) \right]$. 
The primitives in $uv$-space are of the form:
$${ \mathcal{N}(u, v) = \exp\left( -\frac{u^2+v^2}{2} \right) }$$
We render novel views by projecting and alpha-compositing the splats guided by depth distortion 
and normal consistency losses, ensuring that the splats align with the true object surfaces. Once optimized, 
2DGS provides stable depth maps $\{ Z_n \}$ for each viewpoint $n$. For a pixel $(u,v)$ in image $n$, 
its 3D coordinate is:
$$ { \mathbf{X}_n(u,v) = \mathbf{R}_n^{-1} \left( Z_n(u,v)\mathbf{K}_n^{-1}\hat{\mathbf{x}} - \mathbf{t}_n \right) } $$
where $\hat{\mathbf{x}}=[u,v,1]^{\intercal}$ represents homogeneous image coordinates. 2DGS ensures high-fidelity 
depth data for subsequent steps. 
</p>
          </div>
        </div>
      </div>
    <!-- 2DGS -->

    <!-- LangSAM and TSDF -->
    <div class="sec-wrapper blog-sec-wrapper" id="integration">
      <div class="row section-head">
        <div class="col col-wrapper">
          <div class="section-title">Language-Guided Segmentation and Affordance Fusion</div>
        </div>
      </div>
      <div class="row section-item">
        <div class="col col-wrapper">
<p>
Once we have the depth data from 2DGS, we use <a href="https://arxiv.org/abs/2201.03546">LangSAM</a> to produce query and sub-region segmentation masks 
$M_{\text{obj}, n}(u,v), M_{\text{aff}, n}(u,v) \in [0,1]$ for each image $n$ from textual queries (e.g. "grasp the vase handle"). 
Then, we unproject each mask pixel into 3D by taking:
$$ { \mathbf{X}_{\text{obj}, n}(u, v) := \mathbf{X}_n(u,v) \text{ for } \mathbf{M}_{\text{obj}, n}(u, v)=1 }$$
$$ { \mathbf{X}_{\text{aff}, n}(u, v) := \mathbf{X}_n(u,v) \text{ for } \mathbf{M}_{\text{aff}, n}(u, v)=1 }$$
We fuse these points into a TSDF volume. The TSDF at a voxel $\mathbf{x}$ is:
$$ { D(\mathbf{x}) = 
\begin{cases}
\frac{z_{\text{hit}} - z_{\text{surf}}}{\tau} & \text{if } |z_{\text{hit}} - z_{\text{surf}}| < \tau \\
\text{sgn}(z_{\text{hit}} - z_{\text{surf}})  & \text{otherwise}
\end{cases}
} $$

where $z_{\text{surf}}$ is the measured surface depth and $z_{\text{hit}}$ the voxel's projected depth. $\tau$ is a truncation parameter. 
Multiple scans are integrated by a weighted average of TSDF values:
$$ { D'(\mathbf{x}) = \frac{w(\mathbf{x})D(\mathbf{x}) + w'(\tilde{D}(\mathbf{x}))}{w(\mathbf{x})+w'(\mathbf{x})} } $$ 
where $w(\mathbf{x})$ is a weight capturing confidence. Extracting a mesh $\mathcal{M}$ with <a href="https://en.wikipedia.org/wiki/Marching_cubes">Marching Cubes</a>, 
we subsequently construct a sub-region point cloud $\{ \mathbf{X}_{\text{aff},n}\}$ aligned with $\mathcal{M}$, 
so we can assign affordance scores to each vertex $\mathbf{v}$ on $\mathcal{M}$:
$$ { a(\mathbf{v}) := \exp \left( -\frac{\min || \mathbf{v} - \mathbf{a}_j ||^2 }{ 2\sigma^2_a } \right) } $$
where $\mathbf{a}_j$ are affordance points and $\sigma_a$ is a scaling parameter. Then, vertices near the affordance sub-region have 
$a(\mathbf{v})\approx 1$ while distant vertices have $a(\mathbf{v})\approx 0$. This yields an affordance field over $\mathcal{M}$. 
</p>
          </div>
        </div>
      </div>
    <!-- LangSAM and TSDF -->
  

    <!-- GraspNet -->
    <div class="sec-wrapper blog-sec-wrapper" id="integration">
      <div class="row section-head">
        <div class="col col-wrapper">
          <div class="section-title">Semantically-Rich Grasp Generation</div>
        </div>
      </div>
      <div class="row section-item">
        <div class="col col-wrapper">
<p>
Although agnostic to the method of grasp generation, we adopt the <a href="https://graspnet.net">GraspNet baseline</a> for general-purpose 
grasp generation. During inference, we construct a point cloud directly from the 2DGS-reconstructed vertices 
to pass into GraspNet, which exposes the top candidate poses. We use non-maximum suppression to 
eliminate redundant candidates and retain a diverse subset of $K$ near-optimal poses. Finally, we utilize 
TSDF assets to build a 1-to-1 mapping between the point cloud over $\mathcal{M}$ 
and the discretized affordances. We take advantage of the fact that $\mathcal{M}$ and $\mathbf{a}_j$ are jointly reconstructed. 
Therefore, it suffices to compute a 1-nearest-neighbor-search where queries are the $\mathbf{a}_j$ and 
search points are vertices of $\mathcal{M}$. 
To incorporate semantic affordances, we adopt a composite scoring metric for the top $k=1,\dots,K$ grasp candidate defined by:
$$ { \text{score}_k := \text{softmax}(s_k) \cdot a_k } $$
where $s_k$ is the generated GraspNet score and $a_k$ is the affordance score. Good candidates have both high GraspNet score 
(geometric precision and stability) and high affordance score (semantic relevance).
</p>
          </div>
        </div>
      </div>
    <!-- GraspNet -->


    <!-- Drake -->
    <div class="sec-wrapper blog-sec-wrapper" id="integration">
      <div class="row section-head">
        <div class="col col-wrapper">
          <div class="section-title">Simulation via Inverse Kinematics</div>
        </div>
      </div>
      <div class="row section-item">
        <div class="col col-wrapper">
<p>
We simulate the generated grasp poses in Drake by solving for the desired joint configuration $\mathbf{q} \in \mathbb{R}^7$, which should be close to $\mathbf{q}_{\text{nominal}}$. 
on the <a href="https://www.kuka.com/en-us/products/robotics-systems/industrial-robots/lbr-iiwa">KUKA LBR iiwa</a> 
such that the end-effector gripper pose $\mathbf{X}_{WG}=(\mathbf{R}_{WG}, \mathbf{p}_{WG})$ aligns with post-processed GraspNet target pose $\mathbf{X}_{\text{target}}=(\mathbf{R}_{\text{target}}, \mathbf{p}_{\text{target}})$. 
The inverse kinematics formulation is:
$$ { \arg \min_{\mathbf{q}} \, (\mathbf{q} - \mathbf{q}_{\text{nominal}})^{\intercal} (\mathbf{q}-\mathbf{q}_{\text{nominal}}) } $$
$$ { \text{s.t. } \mathbf{p}_{\text{target}} - \epsilon \leq \mathbf{p}_{WG} \leq  \mathbf{p}_{\text{target}} + \epsilon, \arccos \left( \frac{\text{trace}(\mathbf{R}^{\intercal}_{WG} \mathbf{R}_{\text{target}}) - 1}{2} \right) \leq \theta_{\text{bound}} } $$
</p>
          </div>
        </div>
      </div>
    <!-- Drake -->
The optimal trajectory between each pair of sequenced poses is generated with another linear program via <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS-B</a>, 
guided by losses on path smoothness, straightness, and orientation consistency. 
This ensures smooth and collision-free operation for the pick-place-pick-place manipulation sequence.
</p>
<p>
We execute a complex multi-stage pick-and-place manipulation sequence on various reconstructed objects (e.g. vase, mustard bottle, gravy, basket, and toy plane). 
Each trial involves the robot grasping the target object at an initial location, transporting it to a target location, placing it, then reversing 
the process to return the object to its original position. With our system, the combination of geometric precision with 
open-vocabulary semantic guidance leads to grasp proposals qualitatively closer to human intuition, even in complex multi-stage manipulation tasks. 
</p>


    <!-- Applications -->
    <div class="sec-wrapper blog-sec-wrapper" id="integration">
      <div class="row section-head">
        <div class="col col-wrapper">
          <div class="section-title">Applications</div>
        </div>
      </div>
      <div class="row section-item">
        <div class="col col-wrapper">
<p>
<i>SplatGrasp</i> has wide-ranging applications in robotics and computer vision, particularly in scenarios requiring semantic understanding and precise object 
manipulation. In household robotics, our system works toward personalized assistance, such as retrieving objects or handling fragile items based on natural language 
instructions, as we expose finer-grained control by integrating human interaction. Likewise, the ability to combine geometric accuracy with 
language-driven segmentation makes it valuable in healthcare robotics for tasks like surgical tool handling where geometry-only approaches 
fall short in alignment with complex higher-level goals.
</p>
        </div>
      </div>
    </div>
  <!-- Applications -->


  <!-- Related Readings -->
    <div class="sec-wrapper blog-sec-wrapper" id="references">
      <div class="row section-head">
        <div class="col col-wrapper">
          <div class="section-title">Related Readings</div>
        </div>
      </div>
      <div class="row section-item">
        <div class="col col-wrapper">
          <div>
          Splat-MOVER: Multi-Stage, Open-Vocabulary Robotic Manipulation via Editable Gaussian Splatting
          [<a href="https://splatmover.github.io">Homepage</a>]
          [<a href="https://arxiv.org/abs/2301.10622">Paper</a>]
          [<a href="https://github.com/StanfordMSL/Splat-MOVER">Code</a>]
          </div>
          <div>
          Object-Aware Gaussian Splatting for Robotic Manipulation
          [<a href="https://object-aware-gaussian.github.io"]>Homepage</a>]
          [<a href="https://openreview.net/forum?id=gdRI43hDgo">Paper</a>]
          </div>
          <div>
          Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness
          [<a href="https://arxiv.org/abs/2403.10454">Paper</a>]
          </div>
          <div>
          SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering
          [<a href="https://anttwo.github.io/sugar/"]>Homepage</a>]
          [<a href="https://arxiv.org/abs/2311.12775">Paper</a>]
          [<a href="https://github.com/Anttwo/SuGaR"]>Code</a>]
          </div>
        </div>
      </div>
    </div>
  <!-- Related Readings -->

  <!-- Footer -->
    <footer class="citation col-wrapper">
      <p>
        Copyright &copy; <span id="year"></span> Harrison Zhang
        <i class="bi bi-dot"></i>
        <a href="https://accessibility.huit.harvard.edu">Accessibility</a>
      </p>
    </footer>

  </div>

</body>

<script defer src="blog-script.js"></script>

</html>
